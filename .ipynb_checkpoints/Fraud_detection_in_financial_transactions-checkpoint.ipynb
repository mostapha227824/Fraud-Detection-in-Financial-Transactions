{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opl9YeWrPexo"
   },
   "source": [
    "# üïµÔ∏è‚Äç‚ôÇÔ∏è IEEE-CIS Fraud Detection - Team Project\n",
    "\n",
    "Welcome to our team project on **Fraud Detection using the IEEE-CIS Dataset**, a real-world challenge involving identifying fraudulent online transactions.\n",
    "\n",
    "## üßë‚Äçüíª Team Members:\n",
    "- **Mostapha Abdulaziz**\n",
    "- **Ahmed Imad**\n",
    "- **Noha Ashraf**\n",
    "- **Rana Ahmed**\n",
    "- **Sondos Wael**\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Project Objective\n",
    "\n",
    "The objective of this project is to detect fraudulent transactions using the **IEEE-CIS Fraud Detection Dataset**, one of the most comprehensive and anonymized datasets used in real-world financial systems. Through machine learning models and data preprocessing techniques, we aim to build an accurate fraud detection system capable of identifying suspicious activities.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Dataset Overview\n",
    "\n",
    "The dataset is provided by **Vesta Corporation** and hosted on **Kaggle**. It contains **anonymized transactional data** and **user-related identity information**. The dataset is divided into four main files:\n",
    "\n",
    "### 1. `train_transaction.csv` & `test_transaction.csv`\n",
    "These contain transaction-level features such as:\n",
    "- `TransactionID` ‚Äì unique ID for each transaction\n",
    "- `TransactionDT` ‚Äì time in seconds from a reference date\n",
    "- `TransactionAmt` ‚Äì amount of the transaction\n",
    "- `ProductCD`, `card1`‚Äì`card6` ‚Äì payment instruments\n",
    "- `addr1`, `addr2`, `dist1`, `dist2` ‚Äì location and distance metrics\n",
    "- `P_emaildomain`, `R_emaildomain` ‚Äì purchaser & recipient emails\n",
    "- `C1`‚Äì`C14` ‚Äì count-based features (anonymized)\n",
    "- `D1`‚Äì`D15` ‚Äì time deltas from prior events\n",
    "- `M1`‚Äì`M9` ‚Äì matching flags\n",
    "- `V1`‚Äì`V339` ‚Äì PCA-like engineered features\n",
    "- `isFraud` (only in train) ‚Äì target label: 1 if fraudulent, 0 otherwise\n",
    "\n",
    "### 2. `train_identity.csv` & `test_identity.csv`\n",
    "Contain additional information about:\n",
    "- Device details (e.g., `DeviceType`, `DeviceInfo`)\n",
    "- Browser data\n",
    "- Network address and anonymized identity signals (`id_01` to `id_38`)\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Data Cleaning & Preprocessing\n",
    "\n",
    "The dataset requires extensive preprocessing due to:\n",
    "- High number of null values\n",
    "- Anonymized and encoded variables\n",
    "- Mixed data types (numeric, categorical, textual)\n",
    "\n",
    "We'll explore:\n",
    "- Handling missing data\n",
    "- Feature selection and dimensionality reduction\n",
    "- Encoding of categorical variables\n",
    "- Time feature extraction\n",
    "- Device and browser parsing\n",
    "- Merging identity and transaction data\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Our Approach\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)**: Understand distribution, correlations, and missing values.\n",
    "2. **Feature Engineering**: Create new features from device/browser info, time, emails, etc.\n",
    "3. **Modeling**: Train models like XGBoost, LightGBM, and compare results.\n",
    "4. **Evaluation**: Use metrics such as AUC-ROC, precision, recall to assess fraud detection performance.\n",
    "5. **Interpretation**: Analyze important features and understand model decisions.\n",
    "- And more\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Why This Problem Matters\n",
    "\n",
    "Fraud detection is critical for the financial industry. By working on this project, we simulate what it‚Äôs like to deal with:\n",
    "- Imbalanced classification problems\n",
    "- Real-world noise and data anonymization\n",
    "- Behavioral pattern detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHpnILoy6Nts"
   },
   "source": [
    "# Mounting the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydCdjhfpPalL",
    "outputId": "b6992280-caef-42c3-d3d7-d9ff31f12752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiVkw-KN6dDZ"
   },
   "source": [
    "# importing the dataset from kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "p3XMxpsAFa1D",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Installing Miniconda for RAPIDS\n",
    "!wget -nc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "\n",
    "sys.path.append('/usr/local/lib/python3.11/site-packages/')\n",
    "\n",
    "# Installing RAPIDS for CUDA 12.x\n",
    "!conda create -n rapids -c rapidsai -c nvidia -c conda-forge rapids=24.02 python=3.11 cudatoolkit=12.2 --yes\n",
    "!pip install cudf-cu12 cuml-cu12 --extra-index-url=https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuNaNqgHI7kT",
    "outputId": "b2f5e49e-d4ac-46bd-fc6b-091a1575a00b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "CUDA version: 12060\n",
      "cuDF version: 25.02.01\n",
      "cuML version: 25.02.01\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cudf\n",
    "import cuml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "print(f\"GPU available: {cp.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {cp.cuda.runtime.runtimeGetVersion()}\")\n",
    "print(f\"cuDF version: {cudf.__version__}\")\n",
    "print(f\"cuML version: {cuml.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzaLFzPrS6bi",
    "outputId": "b0ea76da-0255-471f-9e0a-251fee4d3541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat 'kaggle.json': No such file or directory\n",
      "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5J_cQtYES7qL",
    "outputId": "148bda96-dcb7-492d-bbb9-fd95a0e9e124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/kaggle\", line 4, in <module>\n",
      "    from kaggle.cli import main\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/__init__.py\", line 6, in <module>\n",
      "    api.authenticate()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 433, in authenticate\n",
      "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
      "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c ieee-fraud-detection -p /content/drive/MyDrive/ieee-fraud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EZtkvwt2TBA3"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('/content/drive/MyDrive/ieee-fraud/ieee-fraud-detection.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/drive/MyDrive/ieee-fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjxmcDsu4V7t"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3XY0ZQq6nqS"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktQ2Iwuy6pYL"
   },
   "source": [
    "# Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4qDj6Daj9QBO"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_path = '/content/drive/MyDrive/ieee-fraud/ieee-fraud-detection.zip'\n",
    "extract_path = '/content/drive/MyDrive/ieee-fraud'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-BzIVe15Cxl",
    "outputId": "31852004-edeb-4364-ec2b-d3abc12cf4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "train_transaction shape: (590540, 394)\n",
      "train_identity shape: (144233, 41)\n",
      "test_transaction shape: (506691, 393)\n",
      "test_identity shape: (141907, 41)\n",
      "card1 in train_transaction: True\n",
      "V-features in train_transaction: 339\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "train_transaction = pd.read_csv('/content/drive/MyDrive/ieee-fraud/train_transaction.csv')\n",
    "train_identity = pd.read_csv('/content/drive/MyDrive/ieee-fraud/train_identity.csv')\n",
    "test_transaction = pd.read_csv('/content/drive/MyDrive/ieee-fraud/test_transaction.csv')\n",
    "test_identity = pd.read_csv('/content/drive/MyDrive/ieee-fraud/test_identity.csv')\n",
    "\n",
    "\n",
    "print(f\"train_transaction shape: {train_transaction.shape}\")\n",
    "print(f\"train_identity shape: {train_identity.shape}\")\n",
    "print(f\"test_transaction shape: {test_transaction.shape}\")\n",
    "print(f\"test_identity shape: {test_identity.shape}\")\n",
    "print(f\"card1 in train_transaction: {'card1' in train_transaction.columns}\")\n",
    "print(f\"V-features in train_transaction: {len([col for col in train_transaction.columns if col.startswith('V')])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cBxUso17Zs2"
   },
   "source": [
    "# Merge transaction and identity datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gymdSWC67Z8e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# normalizing test_identity column names\n",
    "test_identity.columns = [col.replace('-', '_') for col in test_identity.columns]\n",
    "\n",
    "# merging transaction and identity data\n",
    "train = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "test = test_transaction.merge(test_identity, on='TransactionID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9XPC4Xv-cxN"
   },
   "source": [
    "# **Missing Value Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4T5JMZj-dA3",
    "outputId": "4d15065e-c240-49a1-9223-7bf56e14c5fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "null Summary for Train Dataset:\n",
      "       Missing Count  Missing %\n",
      "id_24         585793  99.196159\n",
      "id_25         585408  99.130965\n",
      "id_08         585385  99.127070\n",
      "id_07         585385  99.127070\n",
      "id_21         585381  99.126393\n",
      "id_26         585377  99.125715\n",
      "id_23         585371  99.124699\n",
      "id_22         585371  99.124699\n",
      "id_27         585371  99.124699\n",
      "dist2         552913  93.628374\n",
      "D7            551623  93.409930\n",
      "id_18         545427  92.360721\n",
      "D13           528588  89.509263\n",
      "D14           528353  89.469469\n",
      "D12           525823  89.041047\n",
      "id_04         524216  88.768923\n",
      "id_03         524216  88.768923\n",
      "D6            517353  87.606767\n",
      "id_33         517251  87.589494\n",
      "id_09         515614  87.312290\n",
      "D8            515614  87.312290\n",
      "D9            515614  87.312290\n",
      "id_10         515614  87.312290\n",
      "id_30         512975  86.865411\n",
      "id_32         512954  86.861855\n",
      "id_34         512735  86.824771\n",
      "id_14         510496  86.445626\n",
      "V153          508595  86.123717\n",
      "V141          508595  86.123717\n",
      "V142          508595  86.123717\n",
      "\n",
      "null Summary for Test Dataset:\n",
      "       Missing Count  Missing %\n",
      "id_24         501951  99.064519\n",
      "id_25         501652  99.005508\n",
      "id_26         501644  99.003929\n",
      "id_08         501632  99.001561\n",
      "id_21         501632  99.001561\n",
      "id_07         501632  99.001561\n",
      "id_22         501629  99.000969\n",
      "id_27         501629  99.000969\n",
      "id_23         501629  99.000969\n",
      "dist2         470255  92.809030\n",
      "id_18         455816  89.959364\n",
      "D7            446558  88.132215\n",
      "id_03         440210  86.879380\n",
      "id_04         440210  86.879380\n",
      "D12           437437  86.332104\n",
      "id_30         436032  86.054814\n",
      "id_33         436020  86.052446\n",
      "id_32         436020  86.052446\n",
      "id_14         435334  85.917058\n",
      "id_34         434516  85.755618\n",
      "id_09         432353  85.328731\n",
      "D9            432353  85.328731\n",
      "D8            432353  85.328731\n",
      "id_10         432353  85.328731\n",
      "V156          430906  85.043153\n",
      "V157          430906  85.043153\n",
      "V147          430906  85.043153\n",
      "V146          430906  85.043153\n",
      "V142          430906  85.043153\n",
      "V141          430906  85.043153\n"
     ]
    }
   ],
   "source": [
    "def missing_summary(df, name=''):\n",
    "    # calculating null counts and percentages\n",
    "    nulls = df.isnull().sum()\n",
    "    null_pct = nulls / len(df) * 100\n",
    "    missing_df = pd.DataFrame({'Missing Count': nulls, 'Missing %': null_pct})\n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values(by='Missing %', ascending=False)\n",
    "    print(f\"\\nnull Summary for {name} Dataset:\")\n",
    "    print(missing_df.head(30))\n",
    "    return missing_df\n",
    "\n",
    "\n",
    "train_missing = missing_summary(train, 'Train')\n",
    "test_missing = missing_summary(test, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOLk0l4s-hRP"
   },
   "source": [
    "here we define a function missing_summary to compute and display the count and percentage of missing values for each column in a dataset. It applies this function to both the training and test datasets, printing the top 30 columns with the highest missingness. This helps identify columns with significant nulls (e.g., id_24, id_25) for preprocessing decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAMuoSYW9knM"
   },
   "source": [
    "# **Preprocessing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zC7Hdb4s-A4l",
    "outputId": "b3553c96-a69c-4fa6-8a97-c7c4b38b6668"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking missingness: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 434/434 [00:00<00:00, 458.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: 74 columns\n",
      "card1 present after missingness: True\n",
      "card1 present after alignment: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding null flags: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 45.15it/s]\n",
      "Imputing categoricals: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:02<00:00,  8.97it/s]\n",
      "Imputing numericals: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 339/339 [00:03<00:00, 93.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying PCA to 292 V-features...\n",
      "PCA completed\n",
      "computing correlation matrix...\n",
      "correlation matrix completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checking correlations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 98/98 [00:00<00:00, 11971.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped correlated columns: 14 columns\n",
      "card1 present after correlations: True\n",
      "remaining nulls in train: 29527000\n",
      "remaining nulls in test: 25334550\n",
      "final shape: Train=(590540, 111), Test=(506691, 110)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(train, test):\n",
    "    # dropping columns with >80% missing values unless critical\n",
    "    key_cols = ['DeviceInfo', 'P_emaildomain', 'R_emaildomain', 'D1', 'D2', 'id_01', 'id_02', 'card1']\n",
    "    high_missing_cols = [col for col in tqdm(train.columns, desc=\"Checking missingness\") if train[col].isnull().mean() > 0.8 and col not in key_cols]\n",
    "    train.drop(columns=high_missing_cols, inplace=True)\n",
    "    test.drop(columns=high_missing_cols, inplace=True, errors='ignore')\n",
    "    print(f\"Dropped columns: {len(high_missing_cols)} columns\")\n",
    "    print(f\"card1 present after missingness: {'card1' in train.columns}\")\n",
    "\n",
    "    # aligning train and test columns\n",
    "    common_cols = train.columns.intersection(test.columns)\n",
    "    train = train[common_cols.union(['isFraud'])]\n",
    "    test = test[common_cols]\n",
    "    print(f\"card1 present after alignment: {'card1' in train.columns}\")\n",
    "\n",
    "    # adding null indicator flags for key columns\n",
    "    null_flags_train = {}\n",
    "    null_flags_test = {}\n",
    "    for col in tqdm(key_cols, desc=\"Adding null flags\"):\n",
    "        if col in train.columns and col != 'isFraud' and train[col].isnull().mean() > 0.1:\n",
    "            null_flags_train[f'{col}_missing'] = train[col].isnull().astype(int)\n",
    "            null_flags_test[f'{col}_missing'] = test[col].isnull().astype(int)\n",
    "    train = train.join(pd.DataFrame(null_flags_train, index=train.index))\n",
    "    test = test.join(pd.DataFrame(null_flags_test, index=test.index))\n",
    "\n",
    "    # imputing categorical columns with 'missing'\n",
    "    cat_cols = train.select_dtypes(include=['object']).columns\n",
    "    for col in tqdm(cat_cols, desc=\"Imputing categoricals\"):\n",
    "        train.loc[:, col] = train[col].fillna('missing')\n",
    "        test.loc[:, col] = test[col].fillna('missing')\n",
    "\n",
    "    # imputing numerical columns with median\n",
    "    num_cols = train.select_dtypes(include=['float64', 'int64']).columns.drop(['isFraud'], errors='ignore')\n",
    "    for col in tqdm(num_cols, desc=\"Imputing numericals\"):\n",
    "        median = train[col].median()\n",
    "        train.loc[:, col] = train[col].fillna(median)\n",
    "        test.loc[:, col] = test[col].fillna(median)\n",
    "\n",
    "    # applying PCA to V-features\n",
    "    v_cols = [col for col in train.columns if col.startswith('V') and train[col].nunique() > 1 and train[col].isnull().mean() < 0.9]\n",
    "    if v_cols:\n",
    "        print(f\"applying PCA to {len(v_cols)} V-features...\")\n",
    "        v_data_train = train[v_cols].copy()\n",
    "        v_data_test = test[v_cols].copy()\n",
    "        for col in v_cols:\n",
    "            median = v_data_train[col].median()\n",
    "            v_data_train[col] = v_data_train[col].fillna(median)\n",
    "            v_data_test[col] = v_data_test[col].fillna(median)\n",
    "        if v_data_train.isnull().sum().sum() == 0 and v_data_train.nunique().min() > 1:\n",
    "            pca = PCA(n_components=min(50, len(v_cols)), random_state=42)\n",
    "            train_v_pca = pca.fit_transform(v_data_train)\n",
    "            test_v_pca = pca.transform(v_data_test)\n",
    "            train_v_pca = pd.DataFrame(train_v_pca).fillna(0)\n",
    "            test_v_pca = pd.DataFrame(test_v_pca).fillna(0)\n",
    "            train = train.drop(columns=v_cols).join(pd.DataFrame(train_v_pca, columns=[f'V_pca_{i}' for i in range(train_v_pca.shape[1])], index=train.index))\n",
    "            test = test.drop(columns=v_cols).join(pd.DataFrame(test_v_pca, columns=[f'V_pca_{i}' for i in range(test_v_pca.shape[1])], index=test.index))\n",
    "            print(\"PCA completed\")\n",
    "        else:\n",
    "            print(\"skipping PCA: V-features have insufficient variance or nulls\")\n",
    "            train = train.drop(columns=v_cols)\n",
    "            test = test.drop(columns=v_cols)\n",
    "    else:\n",
    "        print(\"no valid V-features for PCA\")\n",
    "\n",
    "    # engineering feature: missing count per row\n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    train.loc[:, 'missing_count'] = train.isnull().sum(axis=1)\n",
    "    test.loc[:, 'missing_count'] = test.isnull().sum(axis=1)\n",
    "\n",
    "    # dropping highly correlated numerical features with sampling\n",
    "    numeric_df = train.select_dtypes(include=[np.number]).drop(['isFraud'], axis=1, errors='ignore')\n",
    "    if len(numeric_df.columns) > 100:\n",
    "        print(\"campling data for correlation matrix...\")\n",
    "        sample_df = numeric_df.sample(n=10000, random_state=42)\n",
    "    else:\n",
    "        sample_df = numeric_df\n",
    "    print(\"computing correlation matrix...\")\n",
    "    corr_matrix = sample_df.corr().abs()\n",
    "    print(\"correlation matrix completed\")\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    protected_cols = ['card1'] if 'card1' in train.columns else []\n",
    "    to_drop = [col for col in tqdm(upper_triangle.columns, desc=\"checking correlations\") if col not in protected_cols and any(upper_triangle[col] > 0.9)]\n",
    "    train.drop(columns=to_drop, inplace=True)\n",
    "    test.drop(columns=to_drop, inplace=True)\n",
    "    print(f\"dropped correlated columns: {len(to_drop)} columns\")\n",
    "    print(f\"card1 present after correlations: {'card1' in train.columns}\")\n",
    "\n",
    "    print(\"remaining nulls in train:\", train.isnull().sum().sum())\n",
    "    print(\"remaining nulls in test:\", test.isnull().sum().sum())\n",
    "    print(f\"final shape: Train={train.shape}, Test={test.shape}\")\n",
    "\n",
    "    return train, test\n",
    "\n",
    "# preprocessing data\n",
    "train, test = preprocess_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF7U6jGjGtfe"
   },
   "source": [
    "# **Additional Preprocessing and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvCox0NGGtBg",
    "outputId": "2c12bd36-b4e8-44fd-fc54-eb341d58b3dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating card1_fraud_rate feature...\n",
      "final DataFrame Shape: Train=(590540, 61), Test=(506691, 60)\n",
      "remaining nulls in train: 0\n",
      "remaining nulls in test: 9360\n"
     ]
    }
   ],
   "source": [
    "# dropping constant features\n",
    "train = train.loc[:, train.nunique() > 1]\n",
    "test = test.loc[:, test.nunique() > 1]\n",
    "\n",
    "# dropping irrelevant columns\n",
    "train.drop(columns=['TransactionID'], inplace=True, errors='ignore')\n",
    "test.drop(columns=['TransactionID'], inplace=True, errors='ignore')\n",
    "\n",
    "# engineering group-based feature\n",
    "if 'card1' in train.columns and 'card1' in test.columns:\n",
    "    print(\"creating card1_fraud_rate feature...\")\n",
    "    card1_fraud_rate = train.groupby('card1')['isFraud'].mean()\n",
    "    train['card1_fraud_rate'] = train['card1'].map(card1_fraud_rate)\n",
    "    test['card1_fraud_rate'] = test['card1'].map(card1_fraud_rate.fillna(card1_fraud_rate.mean()))\n",
    "else:\n",
    "    print(\"skipping card1_fraud_rate: card1 column missing\")\n",
    "    train['card1_fraud_rate'] = 0\n",
    "    test['card1_fraud_rate'] = 0\n",
    "\n",
    "print(f\"final DataFrame Shape: Train={train.shape}, Test={test.shape}\")\n",
    "print(\"remaining nulls in train:\", train.isnull().sum().sum())\n",
    "print(\"remaining nulls in test:\", test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MI2dB_3gXprf"
   },
   "source": [
    "# Why Cell 4 Left Tons of Missing Values but Cell 5 Nearly Fixed It\n",
    "\n",
    "Wondering why **Cell 4** left a mess of missing values (29.5 million in `train` 25.3 million in `test`) but **Cell 5** dropped those to 0 in `train` and just 9360 in `test`?\n",
    "\n",
    "## Cell 4: The Big Cleanup\n",
    "**What it does**: Cell 4 is like a giant vacuum cleaner for the dataset (434 columns to start) It tries to tidy up by tossing bad columns filling gaps simplifying data and adding new bits Here's what happens\n",
    "\n",
    "- Drops 74 columns with over 80% missing data (like `id_26` 99% empty) Keeps `card1`\n",
    "- Matches train and test columns (except `isFraud` only in `train`)\n",
    "- Adds flags (like `card1_missing`) for missing key data\n",
    "- Fills gaps: text columns (like `ProductCD`) get ‚Äúmissing‚Äù number columns (like `TransactionAmt`) get the median\n",
    "- Squashes 292 V-columns (like `V1` `V2`) into 50 using PCA (math trick)\n",
    "- Adds `missing_count` to track gaps per row\n",
    "- Drops 14 too-similar number columns keeping `card1`\n",
    "\n",
    "**What went wrong**: It should fill all gaps but left 29.5 million in `train` 25.3 million in `test` Why? It only filled `float64` and `int64` number columns PCA or flag columns might be `float32` or `int32` and got skipped Tons of gaps stayed\n",
    "\n",
    "**Result**: we got `train` (590540 rows 111 columns) and `test` (506691 rows 110 columns) but with tons of missing values\n",
    "\n",
    "## Cell 5: Quick Polish\n",
    "**What it does**: Cell 5 is a fast touch-up It simplifies data ditches a useless column and adds a smart feature Here's how\n",
    "\n",
    "- tosses columns with one value or all blanks (useless for fraud detection)\n",
    "- drops `TransactionID` (just an ID)\n",
    "- adds `card1_fraud_rate` showing how often each `card1` is linked to fraud in `train` Tries to apply it to `test`\n",
    "\n",
    "**Why nulls dropped**:\n",
    "- **Train: 29.5 million to 0**:\n",
    "  - The ‚Äútoss boring columns‚Äù step (`train.nunique() > 1`) cut ~50 columns Many had tons of gaps (like `float32` PCA or flags) If a column was mostly empty or had one value it got cut taking all gaps\n",
    "  - `card1_fraud_rate` has no blanks in `train` (uses `train`‚Äôs clean `card1` and `isFraud`)\n",
    "  - Train ends clean with 61 columns 0 nulls\n",
    "- **Test: 25.3 million to 9360**:\n",
    "  - Same thing: Cut similar columns in `test` wiping most of the 25.3 million gaps\n",
    "  - But `card1_fraud_rate` messed up If `test` has `card1` values not in `train` those spots stay empty That‚Äôs the 9360 nulls\n",
    "  - Cell 6 will fix this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kc2W5gSXlkq"
   },
   "source": [
    "# ***Final Null Imputation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PiWH9hkdXlzz",
    "outputId": "9fd52555-9113-4742-fb0b-b6b5b4111a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with nulls in train:\n",
      "Series([], dtype: int64)\n",
      "Columns with nulls in test:\n",
      "card1_fraud_rate    9360\n",
      "dtype: int64\n",
      "imputing card1_fraud_rate in test...\n",
      "remaining nulls in train: 0\n",
      "remaining nulls in test: 0\n",
      "final shape: Train=(590540, 61), Test=(506691, 60)\n",
      "card1 present in train: True\n",
      "isFraud present in train: True\n",
      "card1_fraud_rate present in test: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns with nulls in train:\")\n",
    "print(train.isnull().sum()[train.isnull().sum() > 0])\n",
    "print(\"Columns with nulls in test:\")\n",
    "print(test.isnull().sum()[test.isnull().sum() > 0])\n",
    "\n",
    "# imputing card1_fraud_rate in test\n",
    "if 'card1_fraud_rate' in test.columns:\n",
    "    print(\"imputing card1_fraud_rate in test...\")\n",
    "    test['card1_fraud_rate'] = test['card1_fraud_rate'].fillna(train['card1_fraud_rate'].mean())\n",
    "\n",
    "# imputing numerical columns\n",
    "num_cols = test.select_dtypes(include=['float64', 'float32', 'int64', 'int32']).columns\n",
    "for col in num_cols:\n",
    "    test.loc[:, col] = test[col].fillna(-999)\n",
    "    train.loc[:, col] = train[col].fillna(-999)\n",
    "\n",
    "# imputing categorical columns\n",
    "cat_cols = test.select_dtypes(include=['object']).columns\n",
    "for col in cat_cols:\n",
    "    test.loc[:, col] = test[col].fillna('missing')\n",
    "    train.loc[:, col] = train[col].fillna('missing')\n",
    "\n",
    "print(\"remaining nulls in train:\", train.isnull().sum().sum())\n",
    "print(\"remaining nulls in test:\", test.isnull().sum().sum())\n",
    "print(f\"final shape: Train={train.shape}, Test={test.shape}\")\n",
    "print(f\"card1 present in train: {'card1' in train.columns}\")\n",
    "print(f\"isFraud present in train: {'isFraud' in train.columns}\")\n",
    "print(f\"card1_fraud_rate present in test: {'card1_fraud_rate' in test.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_63AFR0OZkqc"
   },
   "source": [
    "# Preprocessing and nulls handling overview\n",
    "\n",
    "We cleaned up a huge fraud detection dataset (590540 train rows 506691 test rows ~434 columns) to make it ready for modeling.\n",
    "\n",
    "**Cell 1: Load Data**  \n",
    "Loaded train and test data from CSV files (transactions and identity) Checked we got `card1` and V-features Shapes: train (590540 rows 394 columns) test (506691 rows 393 columns)\n",
    "\n",
    "**Cell 2: Merge Data**  \n",
    "Combined transaction and identity data using `TransactionID` Fixed test column names Got train (434 columns) and test (433 columns)\n",
    "\n",
    "**Cell 3: Check Missing Values**  \n",
    "Looked at missing data Found tons (like `id_26` 99% empty) Helped us plan cleanup\n",
    "\n",
    "**Cell 4: Big Cleanup**  \n",
    "Dropped 74 columns with >80% missing Kept `card1` Aligned train/test columns Added flags for missing key data Filled text columns with ‚Äúmissing‚Äù and numbers with medians Squashed 292 V-features into 50 using PCA Added `missing_count` Dropped 14 similar columns Left 29.5M nulls in train 25.3M in test (missed some column types) Got train (111 columns) test (110 columns)\n",
    "\n",
    "**Cell 5: Quick Polish**  \n",
    "Dropped columns with one value or all blanks Ditched `TransactionID` Added `card1_fraud_rate` (fraud likelihood per `card1`) Train nulls dropped to 0 (cut null-heavy columns) Test nulls to 9360 (`card1_fraud_rate` glitch) Got train (61 columns) test (60 columns)\n",
    "\n",
    "**Cell 6: Final Fix**  \n",
    "Fixed 9360 nulls in test‚Äôs `card1_fraud_rate` with train‚Äôs average Filled any stray number/text columns No nulls left Confirmed `card1` `isFraud` `card1_fraud_rate` are there Final shapes: train (590540 rows 61 columns) test (506691 rows 60 columns)\n",
    "\n",
    "**Preprocessing Done**  \n",
    "We went from messy data to clean null-free datasets ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS0dloKjcWGZ"
   },
   "source": [
    "# **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRf9L6CScVCf"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Cell\n",
    "def feature_engineering(df, is_train=True):\n",
    "    \"\"\"\n",
    "    Applies feature engineering to the dataset\n",
    "    Args:\n",
    "        df: DataFrame (train or test)\n",
    "        is_train: Whether this is training data (needed for some target-based features)\n",
    "    Returns:\n",
    "        DataFrame with new features\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"Starting feature engineering...\")\n",
    "    \n",
    "    # ======================\n",
    "    # 1. Time-Based Features\n",
    "    # ======================\n",
    "    if 'TransactionDT' in df.columns:\n",
    "        print(\"Creating time-based features...\")\n",
    "        # Convert seconds to days\n",
    "        df['Transaction_day'] = df['TransactionDT'] // (24*60*60)\n",
    "        \n",
    "        # Time of day features (morning, afternoon, evening, night)\n",
    "        df['Transaction_hour'] = (df['TransactionDT'] % (24*60*60)) / (60*60)\n",
    "        df['Is_night'] = ((df['Transaction_hour'] >= 22) | (df['Transaction_hour'] <= 6)).astype(int)\n",
    "        df['Is_morning'] = ((df['Transaction_hour'] > 6) & (df['Transaction_hour'] <= 12)).astype(int)\n",
    "        df['Is_afternoon'] = ((df['Transaction_hour'] > 12) & (df['Transaction_hour'] <= 18)).astype(int)\n",
    "        df['Is_evening'] = ((df['Transaction_hour'] > 18) & (df['Transaction_hour'] < 22)).astype(int)\n",
    "        \n",
    "        # Weekend flag\n",
    "        # Assuming day 0 was a Monday (you may need to adjust based on actual data)\n",
    "        df['Is_weekend'] = ((df['Transaction_day'] % 7) >= 5).astype(int)\n",
    "    \n",
    "    # ===========================\n",
    "    # 2. Transaction Amount Features\n",
    "    # ===========================\n",
    "    if 'TransactionAmt' in df.columns:\n",
    "        print(\"Creating transaction amount features...\")\n",
    "        # Log transform of amount\n",
    "        df['TransactionAmt_log'] = np.log1p(df['TransactionAmt'])\n",
    "        \n",
    "        # Binned amounts\n",
    "        df['TransactionAmt_bin'] = pd.cut(df['TransactionAmt'], \n",
    "                                         bins=[0, 10, 50, 100, 500, 1000, float('inf')],\n",
    "                                         labels=['0-10', '10-50', '50-100', '100-500', '500-1000', '1000+'])\n",
    "    \n",
    "    # ===========================\n",
    "    # 3. Email Domain Features\n",
    "    # ===========================\n",
    "    for col in ['P_emaildomain', 'R_emaildomain']:\n",
    "        if col in df.columns:\n",
    "            print(f\"Creating features from {col}...\")\n",
    "            # Extract domain provider\n",
    "            df[f'{col}_provider'] = df[col].apply(lambda x: str(x).split('.')[0] if pd.notnull(x) else 'missing')\n",
    "            \n",
    "            # Free email flag (common for fraud)\n",
    "            free_emails = ['gmail', 'yahoo', 'hotmail', 'outlook', 'aol', 'protonmail']\n",
    "            df[f'{col}_is_free'] = df[f'{col}_provider'].isin(free_emails).astype(int)\n",
    "    \n",
    "    # ===========================\n",
    "    # 4. Card Features\n",
    "    # ===========================\n",
    "    if 'card1' in df.columns:\n",
    "        print(\"Creating card features...\")\n",
    "        # Number of transactions per card (only meaningful if we can group across full dataset)\n",
    "        if is_train:\n",
    "            card_counts = df['card1'].value_counts().to_dict()\n",
    "            df['card1_count'] = df['card1'].map(card_counts)\n",
    "        else:\n",
    "            # For test data, we'd need to use counts from training data\n",
    "            # This would need to be handled separately if doing proper train/test split\n",
    "            df['card1_count'] = -1  # Placeholder\n",
    "    \n",
    "    # ===========================\n",
    "    # 5. Device Features\n",
    "    # ===========================\n",
    "    if 'DeviceInfo' in df.columns:\n",
    "        print(\"Creating device features...\")\n",
    "        # Extract device type (simplified)\n",
    "        df['Device_type'] = df['DeviceInfo'].str.split('/', n=1).str[0]\n",
    "        df['Device_type'] = df['Device_type'].str.split(' ', n=1).str[0]\n",
    "        \n",
    "        # Common device flag\n",
    "        top_devices = df['Device_type'].value_counts().head(5).index\n",
    "        df['Is_common_device'] = df['Device_type'].isin(top_devices).astype(int)\n",
    "    \n",
    "    if 'DeviceType' in df.columns:\n",
    "        # Simple binary feature\n",
    "        df['Is_mobile'] = (df['DeviceType'] == 'mobile').astype(int)\n",
    "    \n",
    "    # ===========================\n",
    "    # 6. Interaction Features\n",
    "    # ===========================\n",
    "    if all(col in df.columns for col in ['TransactionAmt', 'card1_count']):\n",
    "        print(\"Creating interaction features...\")\n",
    "        # Amount relative to card's typical transaction\n",
    "        df['Amt_per_card_count'] = df['TransactionAmt'] / (df['card1_count'] + 1)\n",
    "        \n",
    "    if all(col in df.columns for col in ['TransactionAmt', 'Is_night']):\n",
    "        # Nighttime high-value transactions might be suspicious\n",
    "        df['Night_high_value'] = (df['Is_night'] & (df['TransactionAmt'] > 500)).astype(int)\n",
    "    \n",
    "    # ===========================\n",
    "    # 7. Frequency Encoding\n",
    "    # ===========================\n",
    "    print(\"Adding frequency encoding for categoricals...\")\n",
    "    cat_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    for col in cat_cols:\n",
    "        if is_train:\n",
    "            freq = df[col].value_counts(normalize=True).to_dict()\n",
    "            df[f'{col}_freq'] = df[col].map(freq)\n",
    "        else:\n",
    "            # For test data, we'd need to use frequencies from training data\n",
    "            df[f'{col}_freq'] = -1  # Placeholder\n",
    "    \n",
    "    print(\"Feature engineering complete!\")\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"\\nEngineering features for training data...\")\n",
    "train_fe = feature_engineering(train, is_train=True)\n",
    "\n",
    "print(\"\\nEngineering features for test data...\")\n",
    "test_fe = feature_engineering(test, is_train=False)\n",
    "\n",
    "# Verify the results\n",
    "print(\"\\nFeature engineering results:\")\n",
    "print(f\"Train shape after FE: {train_fe.shape}\")\n",
    "print(f\"Test shape after FE: {test_fe.shape}\")\n",
    "print(f\"New features added: {set(train_fe.columns) - set(train.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNX8aESAcPg9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Saving the preprocessed dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLZETt0IcPKP",
    "outputId": "534e294f-c8c5-4a33-d56f-ae3b3ceaa60c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving preprocessed datasets...\n",
      "verifying saved datasets...\n",
      "train saved shape: (590540, 61)\n",
      "test saved shape: (506691, 60)\n",
      "train saved nulls: 0\n",
      "test saved nulls: 0\n",
      "card1 in train: True\n",
      "isFraud in train: True\n",
      "card1_fraud_rate in test: True\n"
     ]
    }
   ],
   "source": [
    "train_path = '/content/drive/MyDrive/ieee-fraud/train_preprocessed.parquet'\n",
    "test_path = '/content/drive/MyDrive/ieee-fraud/test_preprocessed.parquet'\n",
    "print(\"Saving preprocessed datasets...\")\n",
    "train.to_parquet(train_path, index=False)\n",
    "test.to_parquet(test_path, index=False)\n",
    "\n",
    "print(\"verifying saved datasets...\")\n",
    "train_saved = pd.read_parquet(train_path)\n",
    "test_saved = pd.read_parquet(test_path)\n",
    "\n",
    "print(f\"train saved shape: {train_saved.shape}\")\n",
    "print(f\"test saved shape: {test_saved.shape}\")\n",
    "print(f\"train saved nulls: {train_saved.isnull().sum().sum()}\")\n",
    "print(f\"test saved nulls: {test_saved.isnull().sum().sum()}\")\n",
    "print(f\"card1 in train: {'card1' in train_saved.columns}\")\n",
    "print(f\"isFraud in train: {'isFraud' in train_saved.columns}\")\n",
    "print(f\"card1_fraud_rate in test: {'card1_fraud_rate' in test_saved.columns}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
